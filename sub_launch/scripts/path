#!/usr/bin/env python

import argparse
import math
import time

import numpy

import roslib; roslib.load_manifest('sub_launch')
import rospy
import actionlib
from std_msgs.msg import Header
from geometry_msgs.msg import Pose, Quaternion, Point
from nav_msgs.msg import Odometry
from tf import transformations

from uf_common import orientation_helpers
from uf_common.msg import MoveToAction, MoveToGoal, PoseTwist
from uf_common.orientation_helpers import xyz_array, xyzw_array
from object_finder.msg import FindAction, FindGoal


rospy.init_node('path')


visionclient = actionlib.SimpleActionClient('find_down', FindAction)
print 'Waiting for server...'
visionclient.wait_for_server()


client = actionlib.SimpleActionClient('moveto', MoveToAction)
print 'Waiting for server...'
client.wait_for_server()
print 'ok'

current = orientation_helpers.PoseEditor.from_PoseTwistStamped_topic('/trajectory').depth(1)
client.send_goal(current)

orig_orientation = current.orientation

went = False
ignore = False

def got_feedback(fb):
    global current, went, ignore
    obj_pose = orientation_helpers.PoseEditor.from_Pose(goal.header.frame_id, fb.pose)
    # choose 180 deg rotation that minimizes difference from sub's original orientation
    obj_pose = max([obj_pose, obj_pose.turn_right_deg(180)],
        key=lambda p: p.orientation.dot(orig_orientation)**2)
    
    if ignore: return
    print fb
    if (fb.P_within_10cm > .75 or went):
        # approach mode
        if not went:
            went = time.time()
        
        current = current.set_position(obj_pose.position).depth(1)
        current = current.set_orientation(obj_pose.orientation)
        print current.__dict__
        #current = current.set_position(obj_pose.position + obj_pose.forward_vector*2)
        #current = current.look_at(obj_pose.position)
        client.send_goal(current)
        if time.time() > went + 10:
            client.send_goal_and_wait(current)
            visionclient.cancel_goal()
            #client.send_goal_and_wait(current.forward(3))
            rospy.signal_shutdown('reached path')
    else:
        # search mode
        #current = current.look_at(obj_pose.position)
        #client.send_goal(current)
        pass

guess_pos = current.position + -current.body_up_vector * 3

goal = FindGoal()
goal.header.frame_id = current.header.frame_id
goal.type = goal.TYPE_OBJECT
goal.object_filename = roslib.packages.resource_file('auvsi_robosub', 'models', '2012_uf_mock/path.obj')
goal.prior_distribution.pose.position = Point(*guess_pos)
goal.prior_distribution.pose.orientation.w = 1
goal.prior_distribution.covariance[0+6*0] = 1
goal.prior_distribution.covariance[1+6*1] = 1
goal.prior_distribution.covariance[2+6*2] = 1
goal.prior_distribution.covariance[5+6*5] = 100
goal.min_dist = 3
goal.max_dist = 5
goal.is_180z_symmetric = True
visionclient.send_goal(goal, feedback_cb=got_feedback)

rospy.spin()
